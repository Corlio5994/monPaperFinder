# Learning Journey

I started this project without any idea how to construct an AI solution, and with little experience using web APIs or Python libraries to create solutions. Studying for this project via MS Learn has improved my knowledge and skills in these areas dramatically, and given me the confidence to construct my own AI solution (albeit using existing data analysis, pre-processing and training classes and functions). 

After watching and following along with the live tutorials, I decided to begin on this project by completing the General Learning Path on MS Learn. I learnt how to create different classes of machine-learning models using Azure Machine Learning Studio, and the challenges involved with implementing a successful AI strategy across an entire company, which was enlightening! Of particular importance was the exploration of machine learning pipelines in the Machine Learning Studio segment- this learning path gave me a foundational understanding of the order of processes involved in developing a machine-learning solution. 

I started working on the Python learning path next, which ended up taking a lot longer than expected. The first learning path, which taught me how to use Azure Notebooks with Python, was deprecated, and most of the provided data sets were no longer accessible. In order to complete the learning path, I borrowed vaguely similar databases from UCI Machine Learning and from Microsoft open datasets, which taught me a lot about preparing data for use with a given machine-learning model (this took me a *LOOOONNNGG* time, as it was really hard to find relevant datasets... I later discovered how to search meaningfully for datasets on Kaggle, which would've made the process much easier). Finally, I completed the real-world applications pathway, which reintroduced me to object-oriented programming, taught me how to use the Keras library (while introducing me to basic NLP) and gave me an introduction to Flask (I've utilised Node.js for servers before, but this was my first time using Python for a website). 

Once all of this was done, it was time for university to kick into gear, and my life became a lot busier. After an extraodinarily long week, today is the first day on which I've returned to this AI project since Tuesday. Of course, this also means that today is the day I *started* actually building my AI project... I definitely spent too long working through the learning paths. Today, I came up with 10 project ideas, and chose the one that seemed (at the time), the most feasible yet exciting- that's monPaperFinder, the project you're evaluating now. I knew that building this project would require the use of text analysis/Natural Language Processing, but I wasn't sure what would fit my purposes best.
I considered using Text Analytics from Azure Cognitive Services, but found the use cases too specific to apply to my project. I wanted my model to be making specific and predictive choices using the Arxiv database, and for that it was important to build my own solution. 
Although Keras would probably also have offered me the ability to implement the functionality I wanted, I discovered the PyTorch fundamentals learning pathway, and found the Introduction to Natural Language Processing particularly 
relevant. 

The AI part of my program comes primarily from this course, although I also learnt some additional skills from a YouTube tutorial on integrating custom data into PyTorch and a Python notebook on the Kaggle webpage using the same
dataset I elected to use for a similar process. Although I love learning new things, I was conscious that so close to the deadline, learning something entirely new would not be the most shrewd of ideas. However, I was really inspired by the 
sophisticated modelling and analysis that is possible using embeddings, RNNs and TF-IDF representations of words, so I ended up committing to this whole-heartedly anyway! In the process, I developed my understanding of sophisticated machine-learning models extensively, learnt
the basic functionality of PyTorch and furthered my knowledge of data preprocessing, which is vital for all machine-learning operations. Unfortunately, the enormity of the task 
of data preprocessing meant I actually didn't proceed any further with this project, though I'll be sure to work on it more in future, when I have the time!

Although my progress on this project was ultimately impeded by the sheer volume of data I had to use and my lack of experience with handling data-sets of this size, I still attained a wealth of knowledge and experience in Python development 
and machine learning, and I'm excited to learn more as the MSA program continues!
