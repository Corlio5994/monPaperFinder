{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from __future__ import print_function, division\r\n",
    "import os\r\n",
    "import torch\r\n",
    "import pandas as pd\r\n",
    "from skimage import io, transform\r\n",
    "import numpy as np\r\n",
    "import spacy\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import threading\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "from torchvision import transforms, utils\r\n",
    "\r\n",
    "# Ignore warnings\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")\r\n",
    "\r\n",
    "plt.ion()   # interactive mode"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x2ea556d8b50>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torchtext\r\n",
    "\r\n",
    "class Vocabulary: \r\n",
    "    def __init__(self, freq_threshold):\r\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\", }\r\n",
    "        self.stoi = {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3, }\r\n",
    "        self.freq_threshold = freq_threshold\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.itos)\r\n",
    "    \r\n",
    "    @staticmethod\r\n",
    "    def tokenizer_eng(text):\r\n",
    "        tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\r\n",
    "        return tokenizer(text)\r\n",
    "    \r\n",
    "    def build_vocabulary(self, sentence_list):\r\n",
    "        frequencies = {}\r\n",
    "        idx = 4\r\n",
    "        for sentence in sentence_list:\r\n",
    "            for word in self.tokenizer_eng(sentence):\r\n",
    "                frequencies[word] += 1 \r\n",
    "                if frequencies[word] == self.freq_threshold:\r\n",
    "                    self.stoi[word] = idx\r\n",
    "                    self.itos[idx] = word\r\n",
    "                    idx += 1\r\n",
    "    \r\n",
    "    def numericalize(self, text): \r\n",
    "        tokenized_text = self.tokenizer_eng(text)\r\n",
    "        return [self.stoi[_str] if _str in self.stoi else self.stoi[\"<UNK>\"] for _str in tokenized_text]\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class EmbedClassifier(torch.nn.Module):\r\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\r\n",
    "        super().__init__()\r\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim)\r\n",
    "        self.fc = torch.nn.Linear(embed_dim, num_class)\r\n",
    "\r\n",
    "    def forward(self, text, off):\r\n",
    "        x = self.embedding(text, off)\r\n",
    "        return self.fc(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import logging\r\n",
    "logger = logging.getLogger(\"spacy\")\r\n",
    "logger.setLevel(logging.ERROR)\r\n",
    "\r\n",
    "import dask.bag as db\r\n",
    "import json\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "class ArxivDataset(Dataset):\r\n",
    "    \"\"\"Arxiv Papers Dataset.\"\"\"\r\n",
    "\r\n",
    "    def __init__(self, json_file, freq_threshold=5):\r\n",
    "        \"\"\"\r\n",
    "        Args:\r\n",
    "            json_file (string): path to the json_file containing the arxiv metadata\r\n",
    "            transform (callable, optional): Optional transform to be applied\r\n",
    "                on a sample.\r\n",
    "        \"\"\"\r\n",
    "        data_bag = db.read_text(json_file).map(json.loads).compute()\r\n",
    "        data_bag = data_bag[200000:400001]\r\n",
    "        self.df = pd.DataFrame(data_bag)\r\n",
    "        self.abstracts = self.df[\"abstract\"]\r\n",
    "        self.titles = self.df[\"title\"]\r\n",
    "        self.vocab = Vocabulary(freq_threshold)\r\n",
    "        self.vocab.build_vocabulary(self.abstracts.tolist())\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.df)\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        abstract = self.abstracts[idx]\r\n",
    "        title = self.titles[idx]\r\n",
    "\r\n",
    "        numericalized_abstract = [self.vocab.stoi[\"<SOS>\"]]\r\n",
    "        numericalized_abstract += self.vocab.numericalize(abstract)\r\n",
    "        numericalized_abstract.append(self.vocab.stoi[\"<EOS>\"])\r\n",
    "\r\n",
    "        numericalized_title = [self.vocab.stoi[\"<SOS>\"]]\r\n",
    "        numericalized_title += self.vocab.numericalize(title)\r\n",
    "        numericalized_title.append(self.vocab.stoi[\"<EOS>\"])\r\n",
    "\r\n",
    "        return torch.tensor(numericalized_title), torch.tensor(numericalized_abstract)\r\n",
    "    \r\n",
    "def padify(b):\r\n",
    "    # b is the list of tuples of length batch_size\r\n",
    "    #   - first element of a tuple = label, \r\n",
    "    #   - second = feature (text sequence)\r\n",
    "    # build vectorized sequence\r\n",
    "    v = [encode(x[1]) for x in b]\r\n",
    "    # first, compute max length of a sequence in this minibatch\r\n",
    "    l = max(map(len,v))\r\n",
    "    return ( # tuple of two tensors - labels and features\r\n",
    "        torch.LongTensor([t[0]-1 for t in b]),\r\n",
    "        torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\r\n",
    "    )\r\n",
    "\r\n",
    "def offsetify(b):\r\n",
    "    # first, compute data tensor from all sequences\r\n",
    "    x = [torch.tensor(encode(t[1])) for t in b]\r\n",
    "    # now, compute the offsets by accumulating the tensor of sequence lengths\r\n",
    "    o = [0] + [len(t) for t in x]\r\n",
    "    o = torch.tensor(o[:-1]).cumsum(dim=0)\r\n",
    "    return ( \r\n",
    "        torch.LongTensor([t[0]-1 for t in b]), # labels\r\n",
    "        torch.cat(x), # text \r\n",
    "        o\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_loader(json_file, batch_size=100, num_workers=8, shuffle=True, pin_memory=True):\r\n",
    "    dataset = ArxivDataset(json_file)\r\n",
    "    loader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle, pin_memory=pin_memory,\r\n",
    "    collate_fn=padify())\r\n",
    "    return loader\r\n",
    "\r\n",
    "dataloader = get_loader(\"arxiv-metadata-oai-snapshot.json\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}